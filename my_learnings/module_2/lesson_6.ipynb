{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ec6550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Turn 1 ---\n",
      "AI: Hello, Bob! How can I assist you today?\n",
      "(Checking memory: 2 messages)\n",
      "\n",
      "--- Turn 2 ---\n",
      "AI: That's a great choice! Blue is often associated with calmness and serenity. Do you have a specific shade of blue that you prefer?\n",
      "(Checking memory: 4 messages)\n",
      "\n",
      "--- Turn 3 (This will trigger summarization) ---\n",
      "AI: You mentioned that your name is Bob.\n",
      "(Checking memory: 6 messages)\n",
      "\n",
      "--- SUMMARIZING CONVERSATION ---\n",
      "New History: Conversation summary: Bob introduced himself and shared that his favorite color is blue. The AI acknowledged Bob's name, commented on his color choice, and asked if he prefers a specific shade of blue. Bob then asked the AI to recall his name, which the AI correctly did.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'session_id': 'my-chat-session-1',\n",
       " 'input': HumanMessage(content='What did I say my name was?', additional_kwargs={}, response_metadata={})}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "# The import has been updated to the correct location\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# Load API keys and set up tracing\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Intro to LangGraph\"\n",
    "\n",
    "# --- External Memory Store ---\n",
    "# In a real app, this would be a database. We'll use a dictionary to simulate it.\n",
    "memory_store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> ChatMessageHistory:\n",
    "    if session_id not in memory_store:\n",
    "        memory_store[session_id] = ChatMessageHistory()\n",
    "    return memory_store[session_id]\n",
    "\n",
    "# --- Define the State ---\n",
    "# The state is minimal, only holding the session ID and the current input.\n",
    "class GraphState(TypedDict):\n",
    "    session_id: str\n",
    "    input: HumanMessage\n",
    "\n",
    "# --- Define Nodes and Router ---\n",
    "def call_llm(state: GraphState):\n",
    "    session_id = state['session_id']\n",
    "    user_input = state['input']\n",
    "    \n",
    "    # Load history from our external store\n",
    "    memory = get_session_history(session_id)\n",
    "    memory.add_message(user_input)\n",
    "    \n",
    "    # Invoke the model\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    response = llm.invoke(memory.messages)\n",
    "    \n",
    "    # Save the AI's response back to the external store\n",
    "    memory.add_message(response)\n",
    "    print(f\"AI: {response.content}\")\n",
    "    return {} # No need to update state, as memory is external\n",
    "\n",
    "def summarize_conversation(state: GraphState):\n",
    "    print(\"\\n--- SUMMARIZING CONVERSATION ---\")\n",
    "    session_id = state['session_id']\n",
    "    memory = get_session_history(session_id)\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    summary_prompt = \"Summarize this conversation concisely: \" + \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in memory.messages])\n",
    "    summary = llm.invoke(summary_prompt).content\n",
    "    \n",
    "    # Replace the history in the external store with the summary\n",
    "    memory.clear()\n",
    "    memory.add_message(AIMessage(content=f\"Conversation summary: {summary}\"))\n",
    "    print(f\"New History: {memory.messages[-1].content}\")\n",
    "    return {}\n",
    "\n",
    "def should_summarize(state: GraphState):\n",
    "    # The router checks the length of the *external* memory\n",
    "    session_id = state['session_id']\n",
    "    memory = get_session_history(session_id)\n",
    "    print(f\"(Checking memory: {len(memory.messages)} messages)\")\n",
    "    if len(memory.messages) > 4:\n",
    "        return \"summarize_conversation\"\n",
    "    else:\n",
    "        return END\n",
    "\n",
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"conversation\", call_llm)\n",
    "workflow.add_node(\"summarize_conversation\", summarize_conversation)\n",
    "workflow.set_entry_point(\"conversation\")\n",
    "workflow.add_conditional_edges(\"conversation\", should_summarize)\n",
    "workflow.add_edge(\"summarize_conversation\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- Run a simulation of the chatbot ---\n",
    "session_id = \"my-chat-session-1\"\n",
    "\n",
    "print(\"--- Turn 1 ---\")\n",
    "app.invoke({\"session_id\": session_id, \"input\": HumanMessage(content=\"Hi! I'm Bob.\")})\n",
    "\n",
    "print(\"\\n--- Turn 2 ---\")\n",
    "app.invoke({\"session_id\": session_id, \"input\": HumanMessage(content=\"My favorite color is blue.\")})\n",
    "\n",
    "print(\"\\n--- Turn 3 (This will trigger summarization) ---\")\n",
    "app.invoke({\"session_id\": session_id, \"input\": HumanMessage(content=\"What did I say my name was?\")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
