{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdbe0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 multiplied by 5 is 20.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "# Import the required message types\n",
    "from langchain_core.messages import AIMessage, ToolMessage\n",
    "\n",
    "# Load API keys and set up tracing\n",
    "load_dotenv()\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"Intro to LangGraph\"\n",
    "\n",
    "# --- Define Tools ---\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# --- Define State and Nodes ---\n",
    "class GraphState(TypedDict):\n",
    "    messages: list\n",
    "\n",
    "def call_llm(state: GraphState):\n",
    "    llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "    messages = state['messages']\n",
    "    llm_with_tools = llm.bind_tools([multiply])\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": state['messages'] + [response]}\n",
    "\n",
    "def call_tool(state: GraphState):\n",
    "    last_message = state['messages'][-1]\n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    tool_name = tool_call['name']\n",
    "    tool_args = tool_call['args']\n",
    "    \n",
    "    if tool_name == \"multiply\":\n",
    "        result = multiply.invoke(tool_args)\n",
    "\n",
    "    # FIX: Wrap the integer result in a ToolMessage\n",
    "    # The content must be a string, and you must include the tool_call_id\n",
    "    tool_message = ToolMessage(\n",
    "        content=str(result), \n",
    "        tool_call_id=tool_call['id']\n",
    "    )\n",
    "    \n",
    "    return {\"messages\": state['messages'] + [tool_message]}\n",
    "\n",
    "# --- Define the Router (Conditional Edge) ---\n",
    "def should_call_tool(state: GraphState) -> Literal[\"call_tool\", \"__end__\"]:\n",
    "    last_message = state['messages'][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"call_tool\"\n",
    "    return \"__end__\"\n",
    "\n",
    "# --- Build the Graph ---\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node(\"llm\", call_llm)\n",
    "workflow.add_node(\"call_tool\", call_tool)\n",
    "workflow.set_entry_point(\"llm\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_call_tool,\n",
    ")\n",
    "workflow.add_edge(\"call_tool\", \"llm\")\n",
    "app = workflow.compile()\n",
    "\n",
    "# Run the app. The input must be a list of Message objects.\n",
    "final_state = app.invoke({\"messages\": [(\"user\", \"What is 4 * 5?\")]})\n",
    "\n",
    "# Print the final AI message\n",
    "print(final_state['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
